{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "333ae546d607a744",
   "metadata": {},
   "source": [
    "# Workshop: Building an Information Retrieval System for Podcast Episodes\n",
    "\n",
    "## Objective:\n",
    "Create an Information Retrieval (IR) system that processes a dataset of podcast transcripts and, given a query, returns the episodes where the host and guest discuss the query topic. Use TF-IDF and BERT for vector space representation and compare the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b89caf4",
   "metadata": {},
   "source": [
    "### Step 1: Import Libraries\n",
    "Import necessary libraries for data handling, text processing, and machine learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ronny Amores\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd  # Librería para el manejo de datos\n",
    "import string  # Librería para operaciones con cadenas de texto\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer  # Para la representación vectorial TF-IDF\n",
    "from sklearn.metrics.pairwise import cosine_similarity  # Para calcular similitudes de coseno\n",
    "import tensorflow as tf  # Librería de machine learning\n",
    "from transformers import BertTokenizer, TFBertModel  # Modelos preentrenados de BERT\n",
    "import numpy as np  # Librería para operaciones con arrays\n",
    "from nltk.corpus import stopwords  # Para eliminar palabras de parada en inglés\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a2faa7e20eb347",
   "metadata": {},
   "source": [
    "### Step 2: Load the Dataset\n",
    "Load the dataset of podcast transcripts.\n",
    "\n",
    "Find the dataset in: https://www.kaggle.com/datasets/rajneesh231/lex-fridman-podcast-transcript\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e8f07a38e5d7d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id            guest                    title  \\\n",
      "0   1      Max Tegmark                 Life 3.0   \n",
      "1   2    Christof Koch            Consciousness   \n",
      "2   3    Steven Pinker  AI in the Age of Reason   \n",
      "3   4    Yoshua Bengio            Deep Learning   \n",
      "4   5  Vladimir Vapnik     Statistical Learning   \n",
      "\n",
      "                                                text  \n",
      "0  As part of MIT course 6S099, Artificial Genera...  \n",
      "1  As part of MIT course 6S099 on artificial gene...  \n",
      "2  You've studied the human mind, cognition, lang...  \n",
      "3  What difference between biological neural netw...  \n",
      "4  The following is a conversation with Vladimir ...  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data/podcastdata_dataset.csv')  # Cargar el conjunto de datos de las transcripciones de podcasts\n",
    "print(df.head())  # Mostrar las primeras filas del DataFrame para ver la estructura de los datos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d2bdbcfe1c15b1",
   "metadata": {},
   "source": [
    "### Step 3: Text Preprocessing\n",
    "* Delete punctuation\n",
    "* Delete stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96e27a436acfee0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id            guest                    title  \\\n",
      "0   1      Max Tegmark                 Life 3.0   \n",
      "1   2    Christof Koch            Consciousness   \n",
      "2   3    Steven Pinker  AI in the Age of Reason   \n",
      "3   4    Yoshua Bengio            Deep Learning   \n",
      "4   5  Vladimir Vapnik     Statistical Learning   \n",
      "\n",
      "                                                text  \\\n",
      "0  As part of MIT course 6S099, Artificial Genera...   \n",
      "1  As part of MIT course 6S099 on artificial gene...   \n",
      "2  You've studied the human mind, cognition, lang...   \n",
      "3  What difference between biological neural netw...   \n",
      "4  The following is a conversation with Vladimir ...   \n",
      "\n",
      "                                        text_nostopw  \n",
      "0  part mit course 6s099 artificial general intel...  \n",
      "1  part mit course 6s099 artificial general intel...  \n",
      "2  youve studied human mind cognition language vi...  \n",
      "3  difference biological neural networks artifici...  \n",
      "4  following conversation vladimir vapnik hes co ...  \n"
     ]
    }
   ],
   "source": [
    "corpus = df['text']  # Extraer la columna de texto del DataFrame\n",
    "\n",
    "# Eliminar puntuación\n",
    "corpus_nopunct = [doc.lower().translate(str.maketrans('', '', string.punctuation)) for doc in corpus]  # Convertir a minúsculas y eliminar puntuación\n",
    "\n",
    "# Eliminar palabras de parada\n",
    "stopw = set(stopwords.words('english'))  # Obtener el conjunto de palabras de parada en inglés\n",
    "corpus_nostopw = [' '.join([word for word in doc.split() if word not in stopw]) for doc in corpus_nopunct]  # Eliminar palabras de parada\n",
    "\n",
    "df['text_nostopw'] = corpus_nostopw  # Añadir la columna de texto preprocesado al DataFrame\n",
    "print(df.head())  # Mostrar las primeras filas del DataFrame para verificar los cambios\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c778562851705c",
   "metadata": {},
   "source": [
    "### Step 4: Vector Space Representation - TF-IDF\n",
    "Create TF-IDF vector representations of the transcripts.\n",
    "This step transforms the text data into numerical vectors using the TF-IDF method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "194bfa099f153563",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()  # Crear el vectorizador TF-IDF\n",
    "tfidf_mtx = vectorizer.fit_transform(df['text_nostopw'])  # Ajustar y transformar el texto preprocesado en representaciones TF-IDF\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2e67c2290efbd2",
   "metadata": {},
   "source": [
    "### Step 5: Vector Space Representation - BERT\n",
    "Create BERT vector representations of the transcripts using a pre-trained BERT model.\n",
    "This step uses a pre-trained BERT model to transform text data into contextual embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4db1a92bbeb90308",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')  # Cargar el tokenizador preentrenado de BERT\n",
    "model = TFBertModel.from_pretrained('bert-base-uncased')  # Cargar el modelo preentrenado de BERT\n",
    "\n",
    "def generate_bert_embeddings(texts):\n",
    "    \"\"\"Generar representaciones de embeddings BERT para una lista de textos.\"\"\"\n",
    "    embeddings = []  # Lista para almacenar los embeddings\n",
    "    for text in texts:\n",
    "        inputs = tokenizer(text, return_tensors='tf', padding=True, truncation=True)  # Tokenizar el texto\n",
    "        outputs = model(**inputs)  # Obtener las salidas del modelo BERT\n",
    "        embeddings.append(outputs.last_hidden_state[:, 0, :])  # Usar la representación del token [CLS]\n",
    "    return np.array(embeddings).squeeze()  # Devolver los embeddings como un array de numpy\n",
    "\n",
    "corpus_bert = generate_bert_embeddings(corpus_nostopw[:50])  # Generar embeddings BERT para los primeros 50 documentos del corpus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355046ba40ac6a7e",
   "metadata": {},
   "source": [
    "### Step 6: Query Processing\n",
    "Define a function to process the query and compute similarity scores using both TF-IDF and BERT embeddings.\n",
    "This step defines functions to retrieve the top results based on similarity scores for both TF-IDF and BERT representations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40fae62855ac9639",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-04T13:59:13.151983Z",
     "start_time": "2024-07-04T13:59:13.141079Z"
    }
   },
   "outputs": [],
   "source": [
    "def retrieve_tfidf(query):\n",
    "    \"\"\"Recuperar resultados basados en la similitud de TF-IDF.\"\"\"\n",
    "    query_vector = vectorizer.transform([query])  # Transformar la consulta en un vector TF-IDF\n",
    "    similarities = cosine_similarity(tfidf_mtx, query_vector)  # Calcular las similitudes de coseno entre la consulta y el corpus\n",
    "    similarities_df = pd.DataFrame(similarities, columns=['sim'])  # Crear un DataFrame con las similitudes\n",
    "    similarities_df['ep'] = df['title']  # Añadir los títulos de los episodios al DataFrame\n",
    "    return similarities_df  # Devolver el DataFrame de resultados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08e922b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_bert(query):\n",
    "    \"\"\"Recuperar resultados basados en la similitud de BERT.\"\"\"\n",
    "    query_bert = generate_bert_embeddings(query)  # Generar embeddings BERT para la consulta\n",
    "    similarities = cosine_similarity(corpus_bert.reshape(50, 768), query_bert.reshape(1, 768))  # Calcular las similitudes de coseno entre la consulta y el corpus BERT\n",
    "    similarities_df = pd.DataFrame(similarities, columns=['sim'])  # Crear un DataFrame con las similitudes\n",
    "    similarities_df['ep'] = df['title']  # Añadir los títulos de los episodios al DataFrame\n",
    "    return similarities_df  # Devolver el DataFrame de resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140860ab",
   "metadata": {},
   "source": [
    "### Step 7: Retrieve and Compare Results\n",
    "Define a function to retrieve the top results based on similarity scores for both TF-IDF and BERT representations.\n",
    "This step retrieves and compares the results for a given query using both methods.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f20a8c65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Results:\n",
      "           sim                                                 ep\n",
      "109  0.110994                                    Computer Vision\n",
      "70   0.108095  Moore’s Law, Microprocessors, Abstractions, an...\n",
      "236  0.105548                National Institutes of Health (NIH)\n",
      "24   0.104702  Affective Computing, Emotion, Privacy, and Health\n",
      "78   0.101648  Cosmos, Carl Sagan, Voyager, and the Beauty of...\n",
      "217  0.100617  Programming, Algorithms, Hard Problems & the G...\n",
      "72   0.097796                                  Quantum Computing\n",
      "87   0.088561     Evolution, Intelligence, Simulation, and Memes\n",
      "62   0.087663  Algorithms, TeX, Life, and The Art of Computer...\n",
      "41   0.083685  Quantum Mechanics, String Theory, and Black Holes\n",
      "BERT Results:\n",
      "          sim                                                 ep\n",
      "15  0.604827     Reinforcement Learning, Planning, and Robotics\n",
      "11  0.583524                              Poker and Game Theory\n",
      "42  0.579720         Artificial Intelligence: A Modern Approach\n",
      "41  0.578196  Quantum Mechanics, String Theory, and Black Holes\n",
      "39  0.573074                                             iRobot\n",
      "3   0.571954                                      Deep Learning\n",
      "19  0.571945             Generative Adversarial Networks (GANs)\n",
      "18  0.569722                                    Tesla Autopilot\n",
      "37  0.568885                                      Flying Robots\n",
      "44  0.568263  IBM Watson, Jeopardy & Deep Conversations with AI\n"
     ]
    }
   ],
   "source": [
    "query = 'Computer Science'  # Definir una consulta de ejemplo\n",
    "tfidf_results = retrieve_tfidf(query)  # Recuperar resultados utilizando TF-IDF\n",
    "bert_results = retrieve_bert([query])  # Recuperar resultados utilizando BERT\n",
    "\n",
    "# Mostrar los resultados principales para ambas representaciones\n",
    "print(\"TF-IDF Results:\\n\", tfidf_results.sort_values(by='sim', ascending=False).head(10))\n",
    "print(\"BERT Results:\\n\", bert_results.sort_values(by='sim', ascending=False).head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e490241",
   "metadata": {},
   "source": [
    "### Step 8: Test the IR System\n",
    "Test the system with a sample query.\n",
    "Retrieve and display the top results using both TF-IDF and BERT representations.\n",
    "This step validates the retrieval system by testing with a sample query.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6f86c45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Sample Query Results:\n",
      "           sim                                                 ep\n",
      "2    0.205319                            AI in the Age of Reason\n",
      "61   0.160961   Concepts, Analogies, Common Sense & Future of AI\n",
      "119  0.149631                           Measures of Intelligence\n",
      "38   0.142291       Keras, Deep Learning, and the Progress of AI\n",
      "295  0.136549  IQ Tests, Human Intelligence, and Group Differ...\n",
      "12   0.132376                        Brains, Minds, and Machines\n",
      "91   0.101213  Square, Cryptocurrency, and Artificial Intelli...\n",
      "0    0.101050                                           Life 3.0\n",
      "1    0.096279                                      Consciousness\n",
      "75   0.093354   Universal Artificial Intelligence, AIXI, and AGI\n",
      "BERT Sample Query Results:\n",
      "          sim                                                 ep\n",
      "15  0.670569     Reinforcement Learning, Planning, and Robotics\n",
      "3   0.664713                                      Deep Learning\n",
      "42  0.663018         Artificial Intelligence: A Modern Approach\n",
      "39  0.655182                                             iRobot\n",
      "19  0.655083             Generative Adversarial Networks (GANs)\n",
      "2   0.654428                            AI in the Age of Reason\n",
      "38  0.653945       Keras, Deep Learning, and the Progress of AI\n",
      "11  0.650797                              Poker and Game Theory\n",
      "41  0.649503  Quantum Mechanics, String Theory, and Black Holes\n",
      "44  0.648828  IBM Watson, Jeopardy & Deep Conversations with AI\n"
     ]
    }
   ],
   "source": [
    "sample_query = 'Artificial Intelligence'  # Definir una consulta de ejemplo\n",
    "tfidf_results_sample = retrieve_tfidf(sample_query)  # Recuperar resultados utilizando TF-IDF\n",
    "bert_results_sample = retrieve_bert([sample_query])  # Recuperar resultados utilizando BERT\n",
    "\n",
    "# Mostrar los resultados principales para ambas representaciones\n",
    "print(\"TF-IDF Sample Query Results:\\n\", tfidf_results_sample.sort_values(by='sim', ascending=False).head(10))\n",
    "print(\"BERT Sample Query Results:\\n\", bert_results_sample.sort_values(by='sim', ascending=False).head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6f6b6ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Sample Query Results:\n",
      "           sim                                                 ep\n",
      "29   0.280275                                            Spotify\n",
      "135  0.063249                                   Hardcore History\n",
      "192  0.048562  The Existential Threat of Engineered Viruses a...\n",
      "150  0.035882              Speech Recognition with AI and Humans\n",
      "261  0.021342                                         Big Pharma\n",
      "126  0.016533  Conversations, Ideas, Love, Freedom & The Joe ...\n",
      "278  0.015965              Music, AI, and the Future of Humanity\n",
      "38   0.015540       Keras, Deep Learning, and the Progress of AI\n",
      "133  0.013715  On the Nature of Good and Evil, Genius and Mad...\n",
      "65   0.013439      Thinking Fast and Slow, Deep Learning, and AI\n",
      "BERT Sample Query Results:\n",
      "          sim                                              ep\n",
      "42  0.660279      Artificial Intelligence: A Modern Approach\n",
      "11  0.650013                           Poker and Game Theory\n",
      "15  0.649782  Reinforcement Learning, Planning, and Robotics\n",
      "39  0.647950                                          iRobot\n",
      "19  0.642121          Generative Adversarial Networks (GANs)\n",
      "46  0.639317                 Chess, Deep Blue, AI, and Putin\n",
      "34  0.638114     Machines Who Think and the Early Days of AI\n",
      "30  0.637697                                   Microsoft CTO\n",
      "3   0.636238                                   Deep Learning\n",
      "38  0.636197    Keras, Deep Learning, and the Progress of AI\n"
     ]
    }
   ],
   "source": [
    "sample_query = 'Spotify'  # Definir una consulta de ejemplo\n",
    "tfidf_results_sample = retrieve_tfidf(sample_query)  # Recuperar resultados utilizando TF-IDF\n",
    "bert_results_sample = retrieve_bert([sample_query])  # Recuperar resultados utilizando BERT\n",
    "\n",
    "# Mostrar los resultados principales para ambas representaciones\n",
    "print(\"TF-IDF Sample Query Results:\\n\", tfidf_results_sample.sort_values(by='sim', ascending=False).head(10))\n",
    "print(\"BERT Sample Query Results:\\n\", bert_results_sample.sort_values(by='sim', ascending=False).head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890c6f94",
   "metadata": {},
   "source": [
    "### Step 9: Compare Results\n",
    "Analyze and compare the results obtained from TF-IDF and BERT representations.\n",
    "Discuss the differences, strengths, and weaknesses of each method based on the retrieval results.\n",
    "This step involves analyzing the retrieval results and comparing the effectiveness of TF-IDF and BERT methods.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83bb4320",
   "metadata": {},
   "source": [
    "Los resultados de TF-IDF pueden ser menos precisos a comparacion de BERT para consultas complejas debido a la falta de contexto en las representaciones de palabras.\n",
    "BERT, por otro lado, proporciona representaciones contextuales que pueden capturar mejor el significado de las consultas y los documentos, aunque requiere más recursos computacionales, ya que pudimos observar que el tiempo en generar los embeddings de BERT, es mayor que la funcion de TF-IDF\n",
    "\n",
    "En general, BERT tiende a proporcionar mejores resultados en tareas de recuperación de información debido a su capacidad para entender el contexto de las palabras en corpus de grandes cantidades de datos.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
